{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Dataset - Classification of Movie Reviews\n",
    "\n",
    "Let's look at the dataset from the Interneet Movie Database (IMDB) and use it to train a model to classify whether a review is positive or negative.  This is an example of what is called 'sentiment analysis'.  It is used a lot by firms to find out whether their customers are leaving them good or bad reviews, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import IMDB, SST2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUMBER_OF_WORDS = 10000\n",
    "\n",
    "# q_texts = []\n",
    "# q_labels = []\n",
    "\n",
    "# q_test_texts = []\n",
    "# q_test_labels = []\n",
    "\n",
    "# # Test\n",
    "# q_texts = [\"A D\", \"B\", \"A B D\", \"A B C\", \"C\", \"A C\"]\t\n",
    "# q_labels = [1, 0, 1, 1, 0, 1]  # 1 = contains A, 0 = does not contain A\n",
    "\n",
    "# q_test_texts = [\"A\", \"B C\", \"A C D\", \"A B C D\", \"C D\", \"B C D\"]\n",
    "# q_test_labels = [1, 0, 1, 1, 0, 0]  # 1 = contains A, 0 = does not contain A\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_data, test_data = IMDB(split=('train', 'test'))\n",
    "\n",
    "  \n",
    "\n",
    "# count = 0\n",
    "# for item in train_data:\n",
    "#     if item[0] == 1:\n",
    "#         count += 1\n",
    "#         q_texts.append(item[1])\n",
    "#         q_labels.append(item[0] - 1)\n",
    "#     if count>1000:\n",
    "#         break\n",
    "\n",
    "# count = 0\n",
    "# for item in train_data:\n",
    "#     if item[0] == 2:\n",
    "#         count += 1\n",
    "#         q_texts.append(item[1])\n",
    "#         q_labels.append(item[0] - 1)\n",
    "#     if count>1000:\n",
    "#         break\n",
    "\n",
    "# count = 0\n",
    "# for item in test_data:\n",
    "#     if item[0] == 2:\n",
    "#         count += 1\n",
    "#         q_test_texts.append(item[1])\n",
    "#         q_test_labels.append(item[0] - 1)\n",
    "#     if count>200:\n",
    "#         break\n",
    "\n",
    "# count = 0\n",
    "# for item in test_data:\n",
    "#     if item[0] == 1:\n",
    "#         count += 1\n",
    "#         q_test_texts.append(item[1])\n",
    "#         q_test_labels.append(item[0] - 1)\n",
    "#     if count>200:\n",
    "#         break\n",
    "\n",
    "# counter = Counter()\n",
    "# for text in q_texts:\n",
    "#     counter.update(tokenizer(text))\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_data:\n",
    "    counter.update(tokenizer(text[1]))\n",
    "\n",
    "dict = OrderedDict()\n",
    "for i, (token, _) in enumerate(counter.most_common(NUMBER_OF_WORDS)):\n",
    "    dict[token] = i\n",
    "vocab = Vocab(dict)\n",
    "\n",
    "def n_hot_encoding(text, vocab):\n",
    "    indices = [vocab[token] for token in tokenizer(text) if token in vocab]\n",
    "    one_hot = torch.zeros(len(vocab))\n",
    "    one_hot[indices] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImdbClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImdbClassifier1, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUMBER_OF_WORDS, 16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ImdbClassifier1().to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def q_test_model():\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for text, label in zip(q_test_texts, q_test_labels):\n",
    "#             label_tensor = torch.tensor(label).float().to(device)\n",
    "#             label_tensor = label_tensor.unsqueeze(0).unsqueeze(0)\n",
    "#             x = torch.stack([n_hot_encoding(text, vocab)]).to(device)\n",
    "#             y_pred = model(x)\n",
    "#             y_pred =  (y_pred > 0.5).float()    # convert to 1.0 if greater than 0.5, 0.0 otherwise\n",
    "#             correct += (y_pred == label_tensor).sum().item()\n",
    "#             total += 1\n",
    "#         print(f\"Accuracy: {(100.0 * correct)/total if total > 0 else 0.0}\")\n",
    "\n",
    "# def q_train_model():\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     count = 0\n",
    "#     for text, label in zip(q_texts, q_labels):\n",
    "#         optimizer.zero_grad()\n",
    "#         label_tensor = torch.tensor(label).float().to(device)\n",
    "#         label_tensor = label_tensor.unsqueeze(0).unsqueeze(0)\n",
    "#         x = torch.stack([n_hot_encoding(text, vocab)]).to(device)\n",
    "#         y_pred = model(x)\n",
    "#         loss = loss_fn(y_pred, label_tensor)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#         count += 1\n",
    "#     print(f\"Loss: {total_loss/count if count > 0 else -1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_train_model()\n",
    "\n",
    "# q_test_model()\n",
    "# for epoch in range(100):\n",
    "#     q_train_model()\n",
    "#     q_test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.0\n",
      "Epoch:  0\n",
      "390 Loss: 0.3977440000914247\n",
      "Accuracy: 88.776\n",
      "Epoch:  1\n",
      "390 Loss: 0.19219690342814855\n",
      "Accuracy: 88.484\n",
      "Epoch:  2\n",
      "390 Loss: 0.14438770959794503\n",
      "Accuracy: 87.868\n",
      "Epoch:  3\n",
      "390 Loss: 0.11361071500448448\n",
      "Accuracy: 87.284\n",
      "Epoch:  4\n",
      "390 Loss: 0.0974408501330132\n",
      "Accuracy: 87.132\n",
      "Epoch:  5\n",
      "390 Loss: 0.07966549601629162\n",
      "Accuracy: 86.704\n",
      "Epoch:  6\n",
      "390 Loss: 0.06388930152283501\n",
      "Accuracy: 86.452\n",
      "Epoch:  7\n",
      "390 Loss: 0.05257605593291271\n",
      "Accuracy: 86.32\n",
      "Epoch:  8\n",
      "390 Loss: 0.04274060663646873\n",
      "Accuracy: 85.936\n",
      "Epoch:  9\n",
      "390 Loss: 0.035444547758859586\n",
      "Accuracy: 85.84\n",
      "Epoch:  10\n",
      "390 Loss: 0.0311519572032107\n",
      "Accuracy: 85.732\n",
      "Epoch:  11\n",
      "390 Loss: 0.026095112936314887\n",
      "Accuracy: 85.692\n",
      "Epoch:  12\n",
      "390 Loss: 0.0188684296885455\n",
      "Accuracy: 85.412\n",
      "Epoch:  13\n",
      "390 Loss: 0.014275856491196436\n",
      "Accuracy: 85.484\n",
      "Epoch:  14\n",
      "390 Loss: 0.009198761192147008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\ML\\PALE\\models\\imdb_classification\\imdb_part_1.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m\"\u001b[39m, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m train_model()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m test_model()\n",
      "\u001b[1;32md:\\Code\\ML\\PALE\\models\\imdb_classification\\imdb_part_1.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# if labels.sum() < 3:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#     continue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# if labels.sum() > 61:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#     continue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m texts \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([n_hot_encoding(text, vocab) \u001b[39mfor\u001b[39;49;00m text \u001b[39min\u001b[39;49;00m texts])\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/ML/PALE/models/imdb_classification/imdb_part_1.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m y_pred \u001b[39m=\u001b[39m  (y_pred \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()    \u001b[39m# convert to 1.0 if greater than 0.5, 0.0 otherwise\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        labels = batch[0].float().to(device)\n",
    "        labels = (labels - 1.0).unsqueeze(1)  # IMDB dataset has labels 1 and 2, convert them to 0 and 1\n",
    "        texts = batch[1]\n",
    "        x = torch.stack([n_hot_encoding(text, vocab) for text in texts]).to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        count += 1        \n",
    "    print(i, f\"Loss: {total_loss/count if count > 0 else -1}\")\n",
    "        \n",
    "\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            labels = batch[0].float().to(device)\n",
    "            labels = (labels - 1.0).unsqueeze(1)  # IMDB dataset has labels 1 and 2, convert them to 0 and 1\n",
    "\n",
    "            # if labels.sum() < 3:\n",
    "            #     continue\n",
    "            # if labels.sum() > 61:\n",
    "            #     continue\n",
    "\n",
    "            texts = batch[1]\n",
    "            x = torch.stack([n_hot_encoding(text, vocab) for text in texts]).to(device)\n",
    "            y_pred = model(x)\n",
    "            y_pred =  (y_pred > 0.5).float()    # convert to 1.0 if greater than 0.5, 0.0 otherwise\n",
    "            correct += (y_pred == labels).sum().item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        print(f\"Accuracy: {(100.0 * correct)/total if total > 0 else -1}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_model()\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_model()\n",
    "    test_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
