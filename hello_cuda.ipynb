{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we have a graphics card available to us that is CUDA compatible.  CUDA stands for Compute Unified Device Architecture. It is a parallel computing platform and application programming interface (API) model created by NVIDIA, and it can be used to do the sort of calculations that we use for AI much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got CUDA is available above, then you have a CUDA compatible graphics card AND your installation of PyTorch is set up correctly to use it.  If you think you have a modern NVIDIA graphics card but you got CUDA is not available, then you will need to correct your installation, possibly by removing Torch from your environment and installing the CUDA comppatible version from the Torch website.\n",
    "\n",
    "If you don't have a CUDA compatible graphics card, you can still do all the exercises.  Some of them will run more slowly, as all the work has to be done on your CPU, which isn't as quick for some of the calcuations we do for AI.  \n",
    "\n",
    "We use the line below to choose an appropriate device for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the device do some maths for us.  Here's a very simple example.  We create a tensor containing the number 2, and we create a tensor containing the number 4.  Then we move both tensors to the device we want to do the calculation.  If you have a CUDA compatible graphics card, this will be the graphics card.  Then we multiply the two tensors.  This calcuation happens on the graphics card.  \n",
    "\n",
    "In general we want to run AI calculations on a good graphics card, because they are really good at doing lots of small calculations at the same time, very quickly.  Setting up calculations so they can be done together at once is called vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# create two tensors\n",
    "tensor1 = torch.tensor([2])\n",
    "tensor2 = torch.tensor([4])\n",
    "\n",
    "# move tensors to device\n",
    "tensor1 = tensor1.to(device)\n",
    "tensor2 = tensor2.to(device)\n",
    "\n",
    "# multiply tensors\n",
    "result = torch.mul(tensor1, tensor2)\n",
    "\n",
    "# print result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
